{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSE_MSE_VariationLoss.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gDSFnKo6XogZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626193068378,"user_tz":-120,"elapsed":2512,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}},"outputId":"5b6f6898-2707-4051-91cd-68341ca205ac"},"source":["!pip3 install 'tqdm'"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3xULMOQMXtkc","executionInfo":{"status":"ok","timestamp":1626193068378,"user_tz":-120,"elapsed":6,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import numpy as np\n","from PIL import Image\n","import torchvision\n","import torchvision.transforms as transforms\n","import math\n","from sklearn.preprocessing import normalize\n","import copy\n","import torchvision.datasets as dsets\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from torch.utils.data import Subset, DataLoader\n","import random\n","from sklearn.metrics import confusion_matrix \n","import seaborn as sn\n","import pandas as pd\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxlCuiuBXwXp","executionInfo":{"status":"ok","timestamp":1626193068378,"user_tz":-120,"elapsed":5,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}}},"source":["if not os.path.isdir('./OpenWorld'):\n","  !git clone https://github.com/Alessia-Sc/Open-World-Recognition.git OpenWorld\n","\n","from OpenWorld.CIFAR100 import DatasetCifar100\n","from OpenWorld.ResNet import resnet32\n","from tqdm.notebook import tqdm\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wtq3ZErXyEV","executionInfo":{"status":"ok","timestamp":1626193068379,"user_tz":-120,"elapsed":6,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}}},"source":["DEVICE=\"cuda\"\n","TOTAL_CLASSES = 100\n","NUM_CLASSES = 10\n","\n","BATCH_SIZE = 128\n","LR = 2\n","MOMENTUM = 0.9\n","WEIGHT_DECAY = 1e-5\n","\n","NUM_EPOCHS = 70\n","GAMMA = 0.2\n","MILESTONE = [49,63]\n","\n","K=2000"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m5dJrHaWuxiu"},"source":["## Variation III - MSE & MSE"]},{"cell_type":"code","metadata":{"id":"PpPzCMGjux0P","executionInfo":{"status":"ok","timestamp":1626193068769,"user_tz":-120,"elapsed":395,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}}},"source":["class iCaRLNet(): \n","    def __init__(self, feature_size, n_classes, lr=LR, momentum=MOMENTUM, gamma=GAMMA, weight_decay=WEIGHT_DECAY, milestone=MILESTONE, batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS):\n","        # Network architecture\n","        super(iCaRLNet, self).__init__()\n","        self.net = resnet32(num_classes=n_classes)\n","        self.forward = self.net.forward\n","        self.feature_extractor = self.net.get_feat_ext\n","        self.lr = lr\n","        self.gamma = gamma\n","        self.weight_decay = weight_decay\n","        self.milestone = milestone\n","        self.batch_size = batch_size\n","        self.num_epochs = num_epochs\n","        self.n_classes = 0\n","        self.n_known = 0\n","        self.feature_size=feature_size\n","        self.momentum=momentum\n","\n","        self.MSE = nn.MSELoss()\n","\n","        self.compute_means = True\n","        self.exemplar_means = []\n","        self.exemplar_sets = []\n","        \n","    def classify(self, x, cif): \n","        \n","        if self.compute_means:\n","            exemplar_means = []\n","            #EXEMPLARS\n","            for P_y in self.exemplar_sets:  \n","                features = np.zeros((0,self.feature_size)) \n","                sub = Subset(cif, P_y)\n","                dl = DataLoader(sub, batch_size=self.batch_size,shuffle=False, num_workers=4) \n","                with torch.no_grad():\n","                    for ind, ex, lab in dl:\n","                        ex = Variable(ex).cuda()\n","                        feature = self.feature_extractor(ex).data.cpu().numpy()\n","                        feature = normalize(feature, axis=1, norm='l2')\n","                        features = np.concatenate((features,feature), axis=0) \n","\n","                features = torch.tensor(features)\n","                mu_y = features.mean(0).squeeze() \n","                mu_y.data = mu_y.data / torch.norm(mu_y, p=2)  # L2 Normalize\n","                exemplar_means.append(mu_y)\n","\n","            self.exemplar_means = exemplar_means \n","            self.compute_means = False\n","\n","        exemplar_means = self.exemplar_means\n","        means = torch.stack(exemplar_means) \n","        means = torch.stack([means] * self.batch_size)  \n","        means = means.transpose(1, 2)\n","\n","        feature = self.feature_extractor(x) \n","        for i in range(feature.size(0)):  # Normalize\n","            feature.data[i] = feature.data[i] / torch.norm(feature.data[i], p=2)\n","        feature = feature.unsqueeze(2)  \n","        feature = feature.expand_as(means) \n","        feature = feature.cuda()\n","        means = means.cuda()\n","\n","        dists = torch.sqrt((feature - means).pow(2).sum(1)).squeeze()  \n","        #Compute predictions\n","        _, preds = dists.min(1) \n","        return preds\n","\n","\n","    def construct_exemplar_set(self, images, m):\n","        \n","        features = np.zeros((0,self.feature_size))\n","        indices = np.zeros((0), dtype=int)\n","        dl = torch.utils.data.DataLoader(images, batch_size=self.batch_size,shuffle=False, num_workers=4)\n","        with torch.no_grad():\n","          for ind, img, lab in dl:\n","            x = Variable(img).cuda()\n","            feature = self.feature_extractor(x).data.cpu().numpy()\n","            feature = normalize(feature, axis=1, norm='l2')   \n","            features = np.concatenate((features,feature), axis=0)  \n","            indices = np.concatenate((indices,ind), axis=0)\n","\n","        class_mean = np.mean(features, axis=0)\n","        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n","\n","        exemplar_set = []\n","        exemplar_features = np.zeros((0,64))\n","\n","        for k in range(1, int(m)+1):\n","            S = np.sum(exemplar_features, axis=0)\n","            phi = features    \n","            mu = class_mean \n","            mu_p = 1.0 / k * (phi + S)\n","            mu_p = normalize(mu_p, axis=1, norm='l2')\n","            i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n","            exemplar_set.append(indices[i])  \n","            addfeature =  np.expand_dims(features[i], axis=0)\n","            exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n","\n","            features = np.delete(features, i, 0)\n","            indices = np.delete(indices, i, 0)\n","            \n","        self.exemplar_sets.append(exemplar_set) \n","        \n","\n","    def reduce_exemplar_sets(self, m):\n","        for y, P_y in enumerate(self.exemplar_sets):\n","            self.exemplar_sets[y] = P_y[:int(m)]\n","\n","\n","    def exemplarIndexes(self):\n","        Indexes = []\n","        for P_y in self.exemplar_sets:\n","            Indexes += P_y\n","        return Indexes\n","\n","\n","    def update_representation(self, cifar, batchindexes):\n","\n","        prev_model = copy.deepcopy(self)\n","        prev_model = prev_model.net.eval().cuda()\n","        self.net = self.net.cuda()\n","        self.compute_means = True\n","\n","        self.n_classes += 10\n","\n","        # Form combined training set\n","        newindexes = []\n","        if self.n_classes > 10:     \n","            newindexes = self.exemplarIndexes() \n","        newindexes += list(batchindexes)\n","        \n","        reprdata = Subset(cifar, newindexes)\n","\n","        loader = DataLoader(reprdata, batch_size=self.batch_size,shuffle=True, num_workers=4, drop_last=True)\n","\n","        optimizer = optim.SGD(self.net.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n","        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestone, gamma=self.gamma)\n","\n","        for epoch in tqdm(range(self.num_epochs)):\n","            losses = []\n","            for _, images, labels in loader: \n","                labels = torch.tensor([torch.tensor(train_ds.dict_class_label[c.item()]) for c in labels])\n","                images = Variable(torch.FloatTensor(images)).cuda()\n","                labels = Variable(labels).cuda()\n","                optimizer.zero_grad()\n","                g = self.forward(images)\n","\n","\n","                #Classification Loss - MSE\n","                y_hot = F.one_hot(labels, self.n_classes).float().cuda()\n","                out = torch.softmax(g, dim=1)\n","                loss = self.MSE(out[:,self.n_known:self.n_classes], y_hot[:,self.n_known:self.n_classes])\n","               \n","\n","                if self.n_known > 0:\n","                  q = prev_model.forward(images)\n","                  q = torch.sigmoid(q)\n","                  g = torch.sigmoid(g)\n","\n","                  #Distillation Loss for old classes - MSE \n","                  distloss = self.MSE(g[:,:self.n_known],q[:,:self.n_known])\n","              \n","                  #Add the two losses\n","                  loss += distloss\n","                               \n","                losses.append(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","            \n","            scheduler.step()    \n","        "],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-63NqrbWwF1K","executionInfo":{"status":"ok","timestamp":1626193070319,"user_tz":-120,"elapsed":1553,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}},"outputId":"e2bc088f-3c18-4250-c449-1d213a997f5a"},"source":["train_ds = DatasetCifar100(split=\"train\") \n","test_ds = DatasetCifar100(split='test')\n","randomlist = train_ds.classes\n","\n","def compute_accuracy(dl, icarl):\n","    y_pred = []\n","    y_true = []\n","    total = 0.0\n","    correct = 0.0\n","    for _, images, labels in dl:\n","        labels = torch.tensor([torch.tensor(train_ds.dict_class_label[c.item()]) for c in labels])\n","\n","        images = Variable(images).cuda()\n","        preds = icarl.classify(images, train_ds)\n","\n","        total = total + len(labels)\n","        correct += (preds.data.cpu() == labels).sum()\n","\n","      \n","        preds = preds.detach().cpu().numpy()\n","        y_pred_tmp = [p for p in preds]\n","        y_pred.extend(y_pred_tmp)\n","        \n","        labels = labels.detach().cpu().numpy()\n","        labels = labels.tolist()\n","        y_true.extend(labels) \n","        \n","    acc = 100 * correct / total\n","    return acc, y_pred, y_true"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"JqIziOC3wLfi","executionInfo":{"status":"error","timestamp":1626193070553,"user_tz":-120,"elapsed":6,"user":{"displayName":"Lalaurea Didani","photoUrl":"","userId":"03800353919314836035"}},"outputId":"4f80de4b-f803-4906-acde-2eaa214ff44c"},"source":["torch.cuda.current_device()\n","torch.cuda._initialized = True\n","\n","icarl = iCaRLNet(64, TOTAL_CLASSES)\n","icarl.net = icarl.net.to(DEVICE)\n","\n","acc_vect = []\n","\n","for s in range(10):\n","\n","    print(\"\\n\")\n","    print('-' * 80)\n","    print(f\"ITERATION: {(s+1)*10}/100\")\n","\n","    print(\"Loading training examples for classes\", randomlist[s*NUM_CLASSES:s*NUM_CLASSES + NUM_CLASSES])\n","    batchindexes = train_ds.get_indexes_from_labels(randomlist[s*NUM_CLASSES:s*NUM_CLASSES + NUM_CLASSES])\n","    batch = Subset(train_ds, batchindexes)\n","\n","\n","    testindexes = test_ds.get_indexes_from_labels(randomlist[0:s*NUM_CLASSES + NUM_CLASSES])\n","    test_set = Subset(test_ds, testindexes)\n","\n","    print(\"Batch size train: {} - Batch size test: {}\".format(len(batch), len(test_set)))\n","\n","    train_loader = torch.utils.data.DataLoader(batch, batch_size=BATCH_SIZE,shuffle=True, num_workers=4, drop_last=True)\n","    test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4, drop_last=True) #FALSE\n","\n","    icarl.net.train()\n","    icarl.update_representation(train_ds, batchindexes)\n","    icarl.net.eval()\n","    m = K / icarl.n_classes \n","\n","    icarl.reduce_exemplar_sets(m)\n","\n","    for y in randomlist[s*NUM_CLASSES:s*NUM_CLASSES + NUM_CLASSES]:\n","        imagesInd = train_ds.get_indexes_from_labels([y]) \n","        images = Subset(train_ds, imagesInd)\n","        icarl.construct_exemplar_set(images, m) \n","\n","    icarl.n_known = icarl.n_classes\n","\n","    acc, _, _ = compute_accuracy(train_loader, icarl)\n","    print('Train Accuracy: %.2f' % acc)\n","        \n","    acc, test_preds, test_true = compute_accuracy(test_loader, icarl)\n","    print('Test Accuracy: %.2f' % acc)\n","    acc_vect.append(acc)\n"],"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ba34cd84b01f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0micarl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miCaRLNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOTAL_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0micarl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micarl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0macc_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DEVICE' is not defined"]}]}]}